---
title: "Florida Keys Reef Fish Biodiversity"
author: "Megan Hepner"
date: "April 28, 2017"
output: html_document
---

### Inputs 
1. species-trait matrix
1. psu abundance dataframe
1. psu biomass dataframe

### Outputs 
1.  Functional dendogram (Fig: The final ultrametric functional dendrogram for 348 reef fish in the Florida Keys obtained using hierarchial agglometric clustering.)
+ Trait-matrix visualization 

1.  By *strata* for each sampling year (using PSU data)  
+ Species abundance and plot
+ Species biomass and plot
+ species richness and plot
+ Simpson diversity as effective number of species and plot
+ Shannon diversity as effective number of species and plot
+ Functional diversity as effective number of species and plot

1. By *strata and subregion* for each sampling year (using PSU data) 
+ Species abundance and plot
+ Species biomass and plot
+ species richness and plot
+ Simpson diversity as effective number of species and plot
+ Shannon diversity as effective number of species and plot
+ Functional diversity as effective number of species and plot

1. Figure of the FKNMS coral reef tract with strata, subregions, and NTMR 

1. Statistics 
- Interpolate diversity for each year and strata using kriging 
+ Figure. Spatial distribution of each matrices over time. 
- Extract and interpolate the residuals of each index fitted against each other using GAM
+ temporal variable: YEAR
+ spatial variable: Latitude, longitude
+ environmental variable: STRAT, habitat_cd, depth, bottom temperature 
+ management variable: NTMR 
- Calculate partial deviances 

### Reef Visual Census Background 

Reef fish community data used in this study were collected as part of the multi-agency Reef Visual Census (RVC) [Brandt et al., 2009](https://www.coris.noaa.gov/activities/fish_monitoring_protocol/). Fish communities were visually surveyed underwater annually in the mainland FKNMS (Lower Keys, Middle Keys and Upper Keys) from 1999 to 2012, and biennially from 2012 to 2016. The Dry Tortugas region was surveyed from 1999 to 2000 and biennially from 2004 to 2016.     

The RVC uses a stationary point count method within a randomly selected 7.5 m radius circular plot with a 200m map grid from 1999 to 2012 and 100m map grid from 2014 to 2016 to optimize the observation of conspicuous and diurnally active reef fish, specifically economically and ecologically important reef fish species [Bohnsack and Bannerot, 1986](https://docs.lib.noaa.gov/noaa_documents/NMFS/TR_NMFS/TR_NMFS_41.pdf). 

### Objective 

To analyze and compare changes in species richness, Simpson diversity, Shannon diversity and functional diversity in the Florida Keys National Marine Sanctuary (FKNMS) to reveal changes in temporal and spatial variability from 1999 â€“ 2016. The indices were assessed throughout the Florida Keys, between the upper and lower keys, between habitat types, and for different levels of management (Ecological Reserves (ER), Sanctuary Preservations Areas (SPA), and Special Use/Research Only Areas (SU/RO)). 

## Diversity indicse as effective number of species 

All indices were computed as effective number of species. Effective number of species is the number of equally abundance species needed to produce the observed value of a diversity index (Jost, 2006; Jost et al., 2010; MacArthur, 1965). Jost, 2006 refers to these as true measures of diversity, where prior to the conversion the diversity indices are simply measures of entropy. 

- Species Richness is the number of species in a habitat sampled  

$Richness = \sum_{i=1}^S p_i^0$  

- Simpson diversity measures the probability that two individuals randomly selected from a sample will belong to different species.   

$Simpson = 1 - \sum_{i=1}^S p_i^2$  

- Shannon diveristy is a measure of entropy, it is the uncertainty in the species identity of a sample  

$Shannon = - \sum_{i=1}^S p_i \log_b p_i$  

- Functional diversity (Rao's Q) is a measure of pairwise functional differences between species weighted by their relative abundances   

$Rao Q = \sum_{i=1}^S-1 \sum_{j=i+1}^S d_i p_i p_j$

```{r setup, include= F}
knitr::opts_chunk$set(echo = T)
#rm(list = ls())

#install.packages('devtools')
#devtools::install_github('jeremiaheb/rvc')
library(rvc) #calls RVC data from SEFSC server 
#install.packages("tidyverse")
library(tidyverse)
#install.packages("vegan")
library(vegan) # biodiveristy functions for richness, simpson, and shannon 
#install.packages("dygraphs")
library(dygraphs)
#install.packages("plotly")
#library(plotly)
#install.packages("webshot")
library(webshot)
#install.packages("htmlwidgets")
library(htmlwidgets)
#install.packages("FD")
library(FD)
#install.packages("iNEXT")
library(iNEXT) 
RVCdata_FK = read_rds('big_csv/RVCdata_FK.rds')

map = purrr::map # override maps::map
select = dplyr::select #override MASS::select 
group_by =  dplyr::group_by #override plotly::group_by
summarise = dplyr::summarise #override plotly::summarise
```

## Functional distance trait based matrix (from Lefcheck et al., 2014 ChessMap.Rmd)

Gowdis computes the Gower (1971) similarity coefficient exactly as described by Podani (1999), then converts it to a dissimilarity coefficient by using D = 1 - S

```{r Functional distance matrix, eval = F}
knitr::opts_chunk$set(echo = TRUE)

#install.packages("clue")
library(clue) #calls cl_ultrametric 
#install.packages("fpc")
library(fpc) #calls pamk
#install.packages("ape")
library(ape)
#install.packages("phytools")
library(phytools)
#install.packages("FD")
library(FD)
#biocLite("BiocUpgrade") ## you may need this
#biocLite("ggtree", type = "source")
library(tidyverse)

func_dist_rds <- "functional_diversity/func.dist.rds"

if (!file.exists(func_dist_rds)){
  
  trait_matrix = read_csv('functional_diversity/species_trait_matrix_348_spp.csv')
  
  trait_matrix = trait_matrix %>%
    as.tibble() %>%
    mutate(
      SPECIES_CD = toupper(as.character(SPECIES_CD))) %>% 
    arrange(SPECIES_CD)%>% 
    select(SPECIES_CD, Maxlength, Trophic_level, Trophic_group, Water_column, Diel_activity, Substrate_type, Complexity, Gregariousness) %>% #348*9
    group_by(
      SPECIES_CD, Maxlength, Trophic_level, Trophic_group, Water_column, Diel_activity, Substrate_type, Complexity, Gregariousness) %>%
    #summarize(n = n()) %>%
    #select(-n) %>%
    ungroup() %>%
    mutate(
      # ordinal traits
      Complexity = ordered(Complexity, levels=c("Low","Medium","High")),
      Gregariousness = ordered(Gregariousness, levels=c("1","2","3"))) %>%
    as.data.frame()
  
  #Set species names as row.names and remove extra columns
  rownames(trait_matrix)=trait_matrix$SPECIES_CD 
  
  #Calculate Gower distances. Variables have equal weight. 
  traits.dist = gowdis(trait_matrix, ord="podani")
  
  #Use clustering method to produce ultrametric dendrogram because values of Rao's Q can be maximized when fewer than the max number of functional types are present unless distances are ultramtetric 
  
  #to account for sensitivity in clustering use multiple algorithms  (Mouchet et al., 2008) 
  tree_methods = c("single","complete","average","mcquitty","ward.D") #average is best clustering method 
  trees=lapply(tree_methods,  function(i) hclust(traits.dist, method=i))
  par(mfrow=c(3,2))
  for(i in 1:length(trees)) {plot(trees[[i]])} #plots the 5 different kinds of trees 
  
  #convert trees to ultrametric
  trees.ultra= lapply(trees,function(i) cl_ultrametric(as.hclust(i)))
  
  #Plot each tree
  par(mfrow=c(3,2))
  for (i in 1:length(trees.ultra)) {plot(trees.ultra[[i]])} #plots the 5 different kinds of trees 
  
  #Build the consensus tree (Mouchet et al 2008 Oikos)  
  ensemble.trees=cl_ensemble(list=trees) #list of clusterings 
  class(ensemble.trees)
  consensus.tree=cl_consensus(ensemble.trees) #synthesizes the information in the elements of a cluster ensemble into a single clustering 
  par(mar=c(1,1,1,1))
  plot(consensus.tree, horiz=T)
  
  #Calculate dissimilarity values for each tree using 2-norm (Merigot et al 2010 Ecology) to determine which tree best preserves orignial distances 
  #spectral norm (2-norm) of the differences of the ultrametrics
  all.trees=c(trees.ultra,consensus.tree[1])
  names(all.trees)=c(tree_methods,"consensus")
  (trees.dissim=lapply(all.trees,function(i) cl_dissimilarity(i,traits.dist,method="spectral"))) 
  #Cross-dissimilarities using spectral ultrametric distance:
  #Single = 63.17399
  #complete = 116.8516
  #average = 14.99288
  #mcquitty = 36.06599
  #ward.D = 3161.581
  #consensus =  1065.058 (1067.782) (1091.628)
  
  #Identify best tree and isolate
  trees.dissim2=do.call(rbind,trees.dissim)
  min.tree=which.min(trees.dissim2) #average 
  names(all.trees)[min.tree]
  func.dist=all.trees[names(all.trees)==names(all.trees)[min.tree]][[1]]
  
  #Confirm lowest 2-norm value,  spectral norm (2-norm) of the differences if the ultrametrics 
  cl_dissimilarity(func.dist,traits.dist,method="spectral") #Cross-dissimilarities using spectral ultrametric distance:  14.99288
  
  #Scale by the max value so that all values are between 0-1
  func.dist=func.dist/max(func.dist)
  
  saveRDS(func.dist, "functional_diversity/func_dist_rds")
} 
else { #read data 
  func.dist = read_rds("functional_diversity/func_dist_rds")
} 

#Plot the best tree - average 
pdf("functional_dendrogram.pdf", width = 30, height = 70)
par(mfrow=c(1,1))         #nc-by-nr array 
par(mar=c(2,1,0,2))       #number of lines of margins for bottom, left, top, right 
par(pin=c(25,58))         #plot dimensions 
plot(func.dist, horiz=TRUE, main = "Functional Dendrogram", cex = 1, cex.main = 2, cex.lab = 0.5)
dev.off()

#Phylogenetic tree with 348 tips and 347 internal nodes. 
func.dist.phy = as.phylo(as.hclust(func.dist))

#Write newick tree
write.tree(func.dist.phy, "functional_diversity/functional_dendrogram.nwk")
#write.nexus(func.dist.phy, "functional_diversity/functional_dendrogram.nex") 

tree <- read.tree("functional_diversity/functional_dendrogram.nwk")
# List of 4
#  $ edge       : int [1:694, 1:2] 
#  $ Nnode      : int 347
#  $ tip.label  : chr [1:348] 
#  $ edge.length: num
# - attr(*, "class")= chr "phylo"
# - attr(*, "order")= chr "cladewise"

# Phylogenetic tree with 348 tips and 347 internal nodes.
# Tip labels:
# 	KYP_SECT, ALE_CILI, ELO_SAUR, HAR_JAGU, CEN_UNDE, TYL_CROC, ...
# Rooted; includes branch lengths.

plot(tree, label.offset=0.2, edge.width = 2, type = "cladogram")  
nodelabels() #add node numbers
tiplabels() #add tip numbers

ggtree(tree, branch.length = T) +
  ggtitle("Functional Dendogram") +
  geom_tiplab()+
  geom_text(aes(x = branch, label=branch.length), vjust = -.5) +
  theme_tree2()

ggtree(tree, layout= "circular") +
  ggtitle("Functional Dendogram") +
  geom_tiplab(size = 1, aes(angle=angle))+
  theme_tree2()

# transformed to a tidy data.frame by fortify method
tree_data <- fortify(tree) #node; parent; branch.length; x; y; label; isTip; branch; angle 
head(tree_data)

#Visualize species' differences in multivariate trait space
#Perform k-means clustering with no a priori specification for k
traits.kclus= pamk(traits.dist, krange=2:10) 

#krange: integer vector. Numbers of clusters which are to be compared by the average silhouette width criterion.
#traits.kclus$nc = 10 
#traits.kclus$crit 0.0000000 0.2374706 0.2362513 0.2355555 0.2681756 0.2581352 0.2759547 0.2852216 0.3006077 0.3062123

# #Perform multidimensional scaling on functional dendrogram
traits_nmds = metaMDS(traits.dist,k=traits.kclus$nc,trymax=20)
#k = Number of dimensions

#*** No convergence -- monoMDS stopping criteria:
#499: no. of iterations >= maxit
#1: stress ratio > sratmax

# Dimensions: 10 
# Stress:     0.03545178 
# Stress type 1, weak ties
# No convergent solutions - best solution after 500 tries
# Scaling: centring, PC rotation 
# Species: scores missing


# #Plot in two dimensions - doesn't work 
par(mar=c(4,4,1,1))
ordiplot(traits_nmds,type="n")

#Assign colors to different groups
groups=levels(factor(traits.kclus$pamobject$clustering))
points.symbols=15:16
points.colors=c("firebrick3","cornflowerblue")
for(i in seq_along(groups)) {
  points(traits_nmds$points[traits.kclus$pamobject$clustering==groups[i],],
         pch=points.symbols[i],col=points.colors[i],cex=1.4) }
ordispider(traits_nmds,factor(traits.kclus$pamobject$clustering),label=F)
ordihull(traits_nmds,factor(traits.kclus$pamobject$clustering),lty="dotted")
orditorp(traits_nmds,dis="sites",pcex=0,air=0.5,col="grey10",cex=0.8)

```

## Trait matrix visualization 

[http://jkunst.com/r/pokemon-visualize-em-all/]
[https://d3js.org/]
[http://www.buildingwidgets.com/blog/2015/7/22/week-29-d3treer-v2]

```{r trait visualization}

library(tidyverse)
library(treemap)
devtools::install_github("gluc/data.tree")
devtools::install_github("timelyportfolio/d3treeR")
library(d3treeR)
devtools::install_github("GuangchuangYu/ggtree") #requires R>= 3.3.2
library(ggtree)

trait_matrix = read_csv('functional_diversity/species_trait_matrix_348_spp.csv')

trait_matrix_treemap = trait_matrix %>%
  select(Common_name, Maxlength, Trophic_level, Trophic_group, Water_column,Diel_activity,Substrate_type,Complexity, Gregariousness) %>% 
  group_by(Maxlength,
           Trophic_level,
           Trophic_group,
           Water_column,
           Diel_activity,
           Substrate_type,
           Complexity,
           Gregariousness) %>%
  summarise(n = n())%>% 
  ungroup() %>%
  as.data.frame()

treemap2 = 
  d3treeR::d3tree2(
    treemap(
      dtf = trait_matrix_treemap,
      index = c("Trophic_group", "Trophic_level"), #, "Water_column", "Complexity", "Substrate_type", "Diel_activity"),
      vSize = "n", 
      vColor = "Trophic_group",
      type = "index"),
    #title = "Reef Fish Traits",
    #fontsize.title = 14, 
    #algorithm = "squarified"
    rootname = "Species_Traits")

#treemap example 
data(GNI2014)
d3tree2(
  treemap(
    GNI2014
    ,index=c("continent", "iso3")
    ,vSize="population"
    ,vColor="GNI"
    ,type="value"
  )
  , rootname = "World"
)

```

```{r practice FD code}

trait_matrix = read_csv('functional_diversity/species_trait_matrix_348_spp.csv')
trait_matrix = trait_matrix %>%
  as.tibble() %>%
  mutate(
    SPECIES_CD = toupper(as.character(SPECIES_CD))) %>% 
  arrange(SPECIES_CD)

domain_fk_abun <- read_csv('big_csv/abundance_domain/domain_fk_abun.csv', 
                           col_types = cols(
                             YEAR = col_integer(),
                             REGION = col_character(),
                             SPECIES_CD = col_character(),
                             abundance = col_double(),
                             var = col_double(),
                             n = col_integer(),
                             nm = col_integer(),
                             N = col_double(),
                             NM = col_double(),
                             protected_status = col_character()
                           ))

domain_fk_abun_no_spp = domain_fk_abun %>%
  filter(!stringr::str_detect(SPECIES_CD, 'SPE\\.$')) #348 species 

fd_trial = domain_fk_abun_no_spp %>% #15,651 x 10
  filter(protected_status != "all") %>% 
  select(YEAR, protected_status, SPECIES_CD, abundance) %>% #10,434 * 4
  group_by(YEAR, protected_status) %>% ##32 groups (fk 16*2)
  nest(-YEAR) %>%  
  mutate(
    data_wide = map(data, function(x) 
      full_join(x, trait_matrix %>% select(SPECIES_CD), by='SPECIES_CD') %>%
        mutate(abundance = ifelse(is.na(abundance), 0, abundance)))) %>%
  unnest(data_wide) %>% 
  spread(SPECIES_CD, abundance, fill =0) %>% 
  select(-YEAR, -protected_status) %>% 
  as.matrix()

raoQ.2 = 1/(1-apply(fd_trial, 1, function(x) t(x) %*% as.matrix(func.dist) %*% x))

dim(fd_trial) # 32 348
dim(as.matrix(func.dist)) #348 348

#abundance.matrix 348 species (columns) * 32 communities (rows)
#func.dist matrix 348 species (columns) * 348 species (rows)
#can't multiple matrices 

```

### Fetch the RVC data from 
[NOAA's Southeast Fisheries Science Center server](https://grunt.sefsc.noaa.gov/rvc_analysis20/) using the [RVC package](https://github.com/jeremiaheb/rvc)
- Fetched weighted abundance and biomass data by domain, strata, and primary sampling units 

### Sample Data Variables
- PRIMARY_SAMPLE_UNIT: A code indicating the primary sample unit in which a sample was collected.
- YEAR: A number indicating the calendar year.
- MONTH: A number indicating the month of the year.
- DAY: A number indicating the day of the month (EST).
- STATION_NR: A number indicating the secondary sampling unit within a given primary sample unit.
- LAT_DEGREES: Latitude of secondary sampling unit in decimal degrees).
- LON_DEGREES: Longitude of secondary sampling unit in decimal degrees).
- DEPTH: Average depth, in meters, of secondary sampling unit).
- UNDERWATER_VISIBILITY: visibility, in meters, at secondary sampling unit.
- MAPGRID_NR: A number indicating the primary sample unit.  
- HABITAT_CD: A code indicating the habitat type. 
- ZONE_NR: A code indicating the distance offshore: 
+ 1 - Inshore
+ 2 - Midchannel
+ 3 - Offshore
+ 4 - Fore-reef
- SUBREGION_NR: A number indicating the subregion.
- MPA_NR: A number identifying the marine protected area in which the sample was collected. Zero indicates unprotected status)
- SPECIES_NR: A number indicating the species for a sample 
- SPECIES_CD: A code indicating the species for a sample. Consists of the first three letters of the generic name and the first four of the specific name   
- LEN: The length, in cm, of a sample.      
- NUM: The number of individuals of a given species and length observed in a secondary sampling unit
- TIME_SEEN: A number indicating when, during sampling, an individual was observed. 1: In the first five minutes, 2: From 5-10 minutes, 3: After 10 minutes. 
- PROT: A boolean value indicating whether a sample was in a protected area or not 
+ 1 - protected area
+ 0 - not protected 
- STRAT: A code indicating the stratum in which a sample was taken. Differs by region.   
+ FMLR
+ FSLR
+ HRRF
+ INPR
+ MCPR 
+ OFPR
- REGION: A code indicating the region in which a sample was taken.
+ FLA KEYS (florida keys)
+ DRY TORT (dry tortugas)

### Stratum Data Variables 
- REGION 
- YEAR 
- PROT 
- STRAT 
- NTOT: The number of possible primary sample units for a given year, region, stratum, and protected status 
- GRID_SIZE: The length (in meters) to a side of a primary sample unit for a given year, region, stratum, and protected status.

### Taxonomic Data Variables 
- SPECIES_CD
- FAMILY
- SCINAME 
- COMNAME
- LC: Minimum length at capture, in centimeters
- LM: Median length at maturity, in centimeters.
- WLEN_A: The linear coefficient of the allometric growth equation in grams per millimeter (g/mm)
- WLEN_B: The exponential coefficient of the allometric growth equation

### Benthic Data Variables 
- REGION: A code indicating the region. 
- YEAR: A number indicating the calendar year.
- PRIMARY_SAMPLE_UNIT: A code indicating the primary sample unit in which a sample was collected.
- STATION_NR: A number indicating the secondary sampling unit within a given primary sample unit.
- DEPTH: Average depth, in meters, of secondary sampling unit.
- MAX_HARD_RELIEF: The maximum height, in meters, of hard relief (e.g. hard corals, rock).
- MAX_SOFT_RELIEF: The maximum height, in meters, of soft relief (e.g. soft corals, sponges).
- AVG_HARD_RELIEF: The average height, in meters, of hard relief.
- HARD_REL_PCT_0: Percentage of hard relief less than 0.2 meters.
- HARD_REL_PCT_1: Percentage of hard relief between 0.2 and 0.5 meters.
- HARD_REL_PCT_2: Percentage of hard relief between 0.5 and 1.0 meters.
- HARD_REL_PCT_3: Percentage of hard relief between 1.0 and 1.5 meters.
- HARD_REL_PCT_4: Percentage of hard relief greater than 1.5 meters.
- PCT_SAND: Percentage of abiotic cover that is sand.
- PCT_HARD_BOTTOM: Percentage of abiotic cover that is hard bottom.
- PCT_RUBBLE: Percentage of abiotic cover that is rubble.
- PCT_CORAL: Percentage of biotic hardbottom that is coral.
- PCT_OCTO: Percentage of biotic hardbottom that is octocoral.
- PCT_SPONGE: Percentage of biotic hardbottom that is sponge.

### Function: getDomainDensity 
- YEAR
- REGION
- SPECIES_CD
- density: Domain-wide mean density for a stratified random survey  
- var: Variance in average density per secondary sampling unit
- n: Number of primary sampling units sampled
- nm: Number of secondary sampling units sampled
- N: Number of total possible primary sample units
- NM: Number of possible secondary sampling units
- length_class: The length class or bin. Only present if length_bins is not NULL. The notation, [lower, upper), is inclusive of the lower bound, but exclusive of the upper bound
- protected_status: The protected status. Only present if merge_protected is FALSE

### Florida Keys Strata
1. FSLR - Forereef Shallow Linear Reef (<6m)
2. FMLR - Forereef Midchannel Linear Reef (6-18m)
3. FDLR - Forereef Deep Linear Reef (18-33m)
4. INPR - Inshore patch reef
5. MCPR - Midchannel Patch Reef 
6. OFPR - Offshore Patch Reef
7. HRRF - High Relief Reef (Spur and Groove)

### Florida Keys Primary Sampling Unit 

```{r psu abundance}

if(!file.exists("psu_abun_sum_rds")){
  psu_fk_abun <- read_csv("big_csv/abundance_psu/psu_fk_abun.csv",
                          col_types = cols(protected_status = col_character()))
  #dim(psu_fk_abun) #3,643,306 x 10

  psu_fk_abun_no_spp = psu_fk_abun %>%
    filter(!stringr::str_detect(SPECIES_CD, 'SPE\\.$')) 
  #dim(psu_fk_abun_no_spp) #3,379,964 x 10
  
  psu_abun_sum <- psu_fk_abun_no_spp %>%
    group_by(YEAR, PRIMARY_SAMPLE_UNIT, STRAT) %>%
    summarize(abundance_sum = sum(abundance)) 
  #dim(psu_abun_sum) # 5241 x 4 
  
  saveRDS(psu_abun_sum, "psu_abun_sum_rds")
  
  psu_fk_abun_no_spp_merged = psu_fk_abun_no_spp %>%
  filter(stringr::str_detect(protected_status, 'all')) # 1,689,982 x 10 (this is half the number of rows as the original as it should be)

  write_csv(psu_fk_abun_no_spp_merged, "big_csv/abundance_psu/psu_fk_abundance_no_spp_merged.csv")
}

else{
  psu_abun = read_rds("psu_abun_sum_rds")
}

```

```{r psu biomass}

if(!file.exists("psu_bio_sum_rds")){
  psu_fk_bio <- read_csv('big_csv/biomass_psu/psu_fk_biomass.csv', 
                         col_types = cols( protected_status = col_character())) 
  #dim(psu_fk_bio) 3,429,476*10

  #remove spe.  
  psu_fk_bio_no_spp = psu_fk_bio %>%
    filter(!stringr::str_detect(SPECIES_CD, 'SPE\\.$')) #348 species and 
  #dim(psu_fk_bio_no_spp) 3,219,344*10
  
  #summarize 
  psu_bio_sum <- psu_fk_bio_no_spp %>%
    group_by(YEAR, PRIMARY_SAMPLE_UNIT, STRAT, PROT) %>%
    summarize(biomass_sum = sum(biomass)) 
  #dim(psu_bio_sum)  5,241    5
   saveRDS(psu_bio_sum, "psu_bio_sum_rds")

  psu_fk_bio_no_spp_merged = psu_fk_bio_no_spp %>%
    filter(stringr::str_detect(protected_status, 'all')) #1,609,672 x 10 (this is half the number of rows as the original as it should be)
  write_csv(psu_fk_bio_no_spp_merged, "big_csv/biomass_psu/psu_fk_biomass_no_spp_merged.csv")
}

else{ #read data 
  psu_bio = read_rds("psu_bio_sum_rds")
}

```

```{r psu biodiversity}

if(!file.exists("psu_div_rds")){
  
  #read in psu abundance data 
  psu_fk_abun <- read_csv("big_csv/abundance_psu/psu_fk_abun.csv", col_types = cols(protected_status = col_character())) #3,643,306*10
  
  #remove species not identified to species 
  psu_fk_abun_no_spp = psu_fk_abun %>%
    filter(!stringr::str_detect(SPECIES_CD, 'SPE\\.$')) #3,379,964*10 
  
  #read in trait matrix 
  trait_matrix = read_csv('functional_diversity/species_trait_matrix_348_spp.csv')
  
  #trait matrix needs to be alphabetically for FD code 
  trait_matrix = trait_matrix %>%
    as.tibble() %>%
    mutate(
      SPECIES_CD = toupper(as.character(SPECIES_CD))) %>% 
    arrange(SPECIES_CD)
  
  #compute diversity indices by PSU  
  psu_diversity_no_spp_merged = psu_fk_abun_no_spp %>% #tibble: 3,379,964 x 10 %>%
    filter(protected_status != "all") %>% #tibble: 1,689,982 x 10
    select(YEAR, PRIMARY_SAMPLE_UNIT,STRAT,PROT,SPECIES_CD,abundance) %>% #tibble: 1,689,982 x 6
    group_by(YEAR, PRIMARY_SAMPLE_UNIT) %>% #4,839 groups
    nest(-YEAR) %>%
    mutate(
      data_wide = map(data, function(x) 
        full_join(x, trait_matrix %>% select(SPECIES_CD), by='SPECIES_CD') %>%
          mutate(abundance = ifelse(is.na(abundance), 0, abundance)) %>%
          spread(SPECIES_CD, abundance, fill =0)),
      data_wide = map(data, ~ spread(data=.x, SPECIES_CD, abundance, fill =0)), 
      richness = map( #species richness
        data_wide, 
        function(x) specnumber(x %>% select(-STRAT, -PROT))),
      simpson = map( #simpson diversity as effective number of species 
        data_wide,
        function(x) 1/(1 - diversity(x %>% select(-STRAT, -PROT), index = 'simpson'))),
      shannon = map( #shannon diversity as effective number of species 
        data_wide, 
        function(x) exp(diversity(x %>% select(-STRAT, -PROT), index = 'shannon')))) %>% 
    unnest(richness,simpson, shannon)
  
  #get strata and prot information and remove duplicates
  psu_diversity_no_spp_merged_strat = psu_diversity_no_spp_merged %>%
    unnest(data) #tibble: 1,689,982 x 9 
  
  #save dataframe as csv
  write_csv(psu_diversity_no_spp_merged_strat, "big_csv/diversity_without_spe/psu_diversity_no_spp_merged.csv") 
  
  psu_div = psu_diversity_no_spp_merged_strat %>% 
    dplyr::group_by(YEAR, PRIMARY_SAMPLE_UNIT, STRAT, PROT) %>% #5,241*7 #5241 total PSU's over 15 years 
    dplyr::summarize(
      richness = first(richness),
      simpson = first(simpson),
      shannon = first(shannon))

  saveRDS(psu_div, "psu_div_rds")
  
} else{ #read data 
  psu_div = read_rds("psu_div_rds")
}

```

```{r psu abundance plots and maps}

psu_abun <- read_csv('big_csv/abundance_psu/psu_fk_abundance_no_spp_merged.csv', 
                       col_types = cols(
                          YEAR = col_integer(),
                          REGION = col_character(),
                          STRAT = col_character(),
                          PROT = col_integer(),
                          PRIMARY_SAMPLE_UNIT = col_character(),
                          SPECIES_CD = col_character(),
                          m = col_integer(),
                          var = col_double(),
                          abundance = col_double(),
                          protected_status = col_character())) # 1,689,982  x 10

# the psu_abun dataframe has an abundance for each species for each psu so sum abundance across species for each psu...skip this step for diversity indices
psu_abun_sum <- psu_abun %>%
  group_by(YEAR, PRIMARY_SAMPLE_UNIT, STRAT) %>%
  summarize(abundance_sum = sum(abundance)) # 5241 x 4 (5241 = total # of PSUs sampled from 1999 to 2016)
  
# (optional) the psu_bio_sum_YRsubset dataframe produced with the code below has years with low sample sizes (few PSUs)/missing strata removed (i.e., 1999, 2000, and 2004)
psu_abun_sum_YRsubset <- psu_abun_sum %>%
  filter(YEAR!=1999,YEAR!=2000,YEAR!=2004) # 4725 x 4

# 2. Plot data by year and stratum separately and together
# summarize data to get mean and standard error for each year/stratum combination  
psu_abun_forPlot = psu_abun_sum %>% 
  group_by(YEAR, STRAT) %>%
  summarize(
    abundance_mean = mean(abundance_sum),
    abundance_n = length(abundance_sum),
    abundance_sd = sd(abundance_sum),
    abundance_se = abundance_sd / sqrt(abundance_n),
    abundance_min = min(abundance_sum),
    abundance_max = max(abundance_sum))
# plot mean and se across strat and year using bar graphs
# by STRAT
ggplot(psu_abun_forPlot, aes(x=STRAT, y=abundance_mean)) + geom_bar(position=position_dodge(), stat="identity") + geom_errorbar(aes(ymin=abundance_mean-abundance_se, ymax=abundance_mean+abundance_se), width=.2, position=position_dodge(.9)) + theme_classic() + theme(axis.text.x = element_text(angle=90)) + facet_wrap(~YEAR, scales='free_y')
ggsave(file="fk_MeanAbundance_ByStratumForEachYear.pdf", path="big_csv/plots")
# by YEAR
ggplot(psu_abun_forPlot, aes(x=YEAR, y=abundance_mean, group=YEAR)) + geom_bar(position=position_dodge(), stat="identity") + geom_errorbar(aes(ymin=abundance_mean-abundance_se, ymax=abundance_mean+abundance_se), width=.2, position=position_dodge(.9)) + theme_classic() + theme(axis.text.x = element_text(angle=90)) + facet_wrap(~STRAT, scales='free_y')
ggsave(file="fk_MeanAbundance_ByYearForEachStratum.pdf", path="big_csv/plots")
# by STRAT and YEAR
ggplot(psu_abun_forPlot, aes(x=YEAR, y=abundance_mean, colour=STRAT, group=STRAT)) + geom_point() + geom_line() + geom_errorbar(aes(ymin=abundance_mean-abundance_se, ymax=abundance_mean+abundance_se), width=.2)
ggsave(file="fk_MeanAbundance_ByYearANDStratum.pdf", path="big_csv/plots")

# do this for dataframe with subset of years as well
# summarize data to get mean and standard error for each year/stratum combination  
psu_abun_forPlot_YRsubset = psu_abun_sum_YRsubset %>% 
  group_by(YEAR, STRAT) %>%
  summarize(
    abundance_mean = mean(abundance_sum),
    abundance_n = length(abundance_sum),
    abundance_sd = sd(abundance_sum),
    abundance_se = abundance_sd / sqrt(abundance_n),
    abundance_min = min(abundance_sum),
    abundance_max = max(abundance_sum))
# plot mean and se across strat and year using bar graphs
# by STRAT
ggplot(psu_abun_forPlot_YRsubset, aes(x=STRAT, y=abundance_mean)) + geom_bar(position=position_dodge(), stat="identity") + geom_errorbar(aes(ymin=abundance_mean-abundance_se, ymax=abundance_mean+abundance_se), width=.2, position=position_dodge(.9)) + theme_classic() + theme(axis.text.x = element_text(angle=90)) + facet_wrap(~YEAR, scales='free_y')
ggsave(file="fk_MeanAbundance_ByStratumForEachYear_YRsubset.pdf", path="big_csv/plots")
# by YEAR
ggplot(psu_abun_forPlot_YRsubset, aes(x=YEAR, y=abundance_mean, group=YEAR)) + geom_bar(position=position_dodge(), stat="identity") + geom_errorbar(aes(ymin=abundance_mean-abundance_se, ymax=abundance_mean+abundance_se), width=.2, position=position_dodge(.9)) + theme_classic() + theme(axis.text.x = element_text(angle=90)) + facet_wrap(~STRAT, scales='free_y')
ggsave(file="fk_MeanAbundance_ByYearForEachStratum_YRsubset.pdf", path="big_csv/plots")
# by STRAT and YEAR
ggplot(psu_abun_forPlot_YRsubset, aes(x=YEAR, y=abundance_mean, colour=STRAT, group=STRAT)) + geom_point() + geom_line() + geom_errorbar(aes(ymin=abundance_mean-abundance_se, ymax=abundance_mean+abundance_se), width=.2)
ggsave(file="fk_MeanAbundance_ByYearANDStratum_YRsubset.pdf", path="big_csv/plots")

### tried these for abundance but didn't look good (too many very low values mixed with a small number of very high values)
#ggplot(data = psu_abun_sum) + geom_boxplot(mapping = aes(x = STRAT, y = abundance_sum)) + facet_wrap(~YEAR, scales="free_y")
#ggplot(data = psu_abun_sum) + geom_boxplot(mapping = aes(x = YEAR, y = abundance_sum, group = YEAR)) + facet_wrap(~STRAT, scales="free_y")
```

```{r psu biomass plots and maps}

psu_bio <- read_csv('big_csv/biomass_psu/psu_fk_biomass_no_spp_merged.csv', 
                       col_types = cols(
                          YEAR = col_integer(),
                          REGION = col_character(),
                          STRAT = col_character(),
                          PROT = col_integer(),
                          PRIMARY_SAMPLE_UNIT = col_character(),
                          SPECIES_CD = col_character(),
                          m = col_integer(),
                          var = col_double(),
                          biomass = col_double(),
                          protected_status = col_character())) # 1,609,672  x 10

# the psu_bio dataframe has a biomass for each species for each psu so sum biomass across species for each psu...skip this step for diversity indices
psu_bio_sum <- psu_bio %>%
  group_by(YEAR, PRIMARY_SAMPLE_UNIT, STRAT) %>%
  summarize(biomass_sum = sum(biomass)) # 5241 x 4 (5241 = total # of PSUs sampled from 1999 to 2016)
  
# (optional) the psu_bio_sum_YRsubset dataframe produced with the code below has years with low sample sizes (few PSUs)/missing strata removed (i.e., 1999, 2000, and 2004)
psu_bio_sum_YRsubset <- psu_bio_sum %>%
  filter(YEAR!=1999,YEAR!=2000,YEAR!=2004) # 4725 x 4

# plot data by year and stratum separately and together
# summarize data to get mean and standard error for each year/stratum combination  
psu_bio_forPlot = psu_bio_sum %>% 
  group_by(YEAR, STRAT) %>%
  summarize(
    biomass_mean = mean(biomass_sum),
    biomass_n = length(biomass_sum),
    biomass_sd = sd(biomass_sum),
    biomass_se = biomass_sd / sqrt(biomass_n),
    biomass_min = min(biomass_sum),
    biomass_max = max(biomass_sum))
# plot mean and se across strat and year using bar graphs
# by STRAT
ggplot(psu_bio_forPlot, aes(x=STRAT, y=biomass_mean)) + geom_bar(position=position_dodge(), stat="identity") + geom_errorbar(aes(ymin=biomass_mean-biomass_se, ymax=biomass_mean+biomass_se), width=.2, position=position_dodge(.9)) + theme_classic() + theme(axis.text.x = element_text(angle=90)) + facet_wrap(~YEAR, scales='free_y')
ggsave(file="fk_MeanBiomass_ByStratumForEachYear.pdf", path="big_csv/plots")
# by YEAR
ggplot(psu_bio_forPlot, aes(x=YEAR, y=biomass_mean, group=YEAR)) + geom_bar(position=position_dodge(), stat="identity") + geom_errorbar(aes(ymin=biomass_mean-biomass_se, ymax=biomass_mean+biomass_se), width=.2, position=position_dodge(.9)) + theme_classic() + theme(axis.text.x = element_text(angle=90)) + facet_wrap(~STRAT, scales='free_y')
ggsave(file="fk_MeanBiomass_ByYearForEachStratum.pdf", path="big_csv/plots")
# by STRAT and YEAR
ggplot(psu_bio_forPlot, aes(x=YEAR, y=biomass_mean, colour=STRAT, group=STRAT)) + geom_point() + geom_line() + geom_errorbar(aes(ymin=biomass_mean-biomass_se, ymax=biomass_mean+biomass_se), width=.2)
ggsave(file="fk_MeanBiomass_ByYearANDStratum.pdf", path="big_csv/plots")

# do this for dataframe with subset of years as well
# summarize data to get mean and standard error for each year/stratum combination  
psu_bio_forPlot_YRsubset = psu_bio_sum_YRsubset %>% 
  group_by(YEAR, STRAT) %>%
  summarize(
    biomass_mean = mean(biomass_sum),
    biomass_n = length(biomass_sum),
    biomass_sd = sd(biomass_sum),
    biomass_se = biomass_sd / sqrt(biomass_n),
    biomass_min = min(biomass_sum),
    biomass_max = max(biomass_sum))
# plot mean and se across strat and year using bar graphs
# by STRAT
ggplot(psu_bio_forPlot_YRsubset, aes(x=STRAT, y=biomass_mean)) + geom_bar(position=position_dodge(), stat="identity") + geom_errorbar(aes(ymin=biomass_mean-biomass_se, ymax=biomass_mean+biomass_se), width=.2, position=position_dodge(.9)) + theme_classic() + theme(axis.text.x = element_text(angle=90)) + facet_wrap(~YEAR, scales='free_y')
ggsave(file="fk_MeanBiomass_ByStratumForEachYear_YRsubset.pdf", path="big_csv/plots")
# by YEAR
ggplot(psu_bio_forPlot_YRsubset, aes(x=YEAR, y=biomass_mean, group=YEAR)) + geom_bar(position=position_dodge(), stat="identity") + geom_errorbar(aes(ymin=biomass_mean-biomass_se, ymax=biomass_mean+biomass_se), width=.2, position=position_dodge(.9)) + theme_classic() + theme(axis.text.x = element_text(angle=90)) + facet_wrap(~STRAT, scales='free_y')
ggsave(file="fk_MeanBiomass_ByYearForEachStratum_YRsubset.pdf", path="big_csv/plots")
# by STRAT and YEAR
ggplot(psu_bio_forPlot_YRsubset, aes(x=YEAR, y=biomass_mean, colour=STRAT, group=STRAT)) + geom_point() + geom_line() + geom_errorbar(aes(ymin=biomass_mean-biomass_se, ymax=biomass_mean+biomass_se), width=.2)
ggsave(file="fk_MeanBiomass_ByYearANDStratum_YRsubset.pdf", path="big_csv/plots")

### tried using boxplots but they don't look good
#ggplot(data = psu_bio_sum) + geom_boxplot(mapping = aes(x = STRAT, y = biomass_sum)) + facet_wrap(~YEAR, scales="free_y")
#ggplot(data = psu_bio_sum) + geom_boxplot(mapping = aes(x = YEAR, y = biomass_sum, group = YEAR)) + facet_wrap(~STRAT, scales="free_y")

```

```{r psu biodiversity plots and maps}

psu_diversity = read_csv("big_csv/diversity_without_spe/psu_diversity_no_spp_merged.csv",
                      col_types = cols(
                          YEAR = col_integer(),
                          PRIMARY_SAMPLE_UNIT = col_character(),
                          richness = col_integer(),
                          simpson = col_double(),
                          shannon = col_double(),
                          STRAT = col_character(),
                          SPECIES_CD = col_character(),
                          abun_merged = col_double())) # 1,689,982 x 9

# the psu_diversity dataframe has a value for each diversity metric that is repeated for each YEAR/PSU for each species record....need to remove duplicate rows and can also select columns of interest for the analysis of diversity metrics
psu_div = psu_diversity %>% 
  select(YEAR, PRIMARY_SAMPLE_UNIT, STRAT, richness, simpson, shannon) %>%
  group_by(YEAR, PRIMARY_SAMPLE_UNIT, STRAT) %>%
  summarize(
    richness = first(richness),
    simpson = first(simpson),
    shannon = first(shannon)) # 5241 x 6
  
# (optional) the psu_div_YRsubset dataframe produced with the code below has years with low sample sizes (few PSUs)/missing strata removed (i.e., 1999, 2000, and 2004)
psu_div_YRsubset <- psu_div %>%
  filter(YEAR!=1999,YEAR!=2000,YEAR!=2004) # 4725 x 6


# plot data by year and stratum separately and together
# summarize data to get mean and standard error for each year/stratum combination  
psu_div_forPlot = psu_div %>% 
  group_by(YEAR, STRAT) %>%
  summarize(
    richness_mean = mean(richness),
    richness_n = length(richness),
    richness_sd = sd(richness),
    richness_se = richness_sd / sqrt(richness_n),
    richness_min = min(richness),
    richness_max = max(richness),
    simpson_mean = mean(simpson),
    simpson_n = length(simpson),
    simpson_sd = sd(simpson),
    simpson_se = simpson_sd / sqrt(simpson_n),
    simpson_min = min(simpson),
    simpson_max = max(simpson),
    shannon_mean = mean(shannon),
    shannon_n = length(shannon),
    shannon_sd = sd(shannon),
    shannon_se = shannon_sd / sqrt(shannon_n),
    shannon_min = min(shannon),
    shannon_max = max(shannon))

write_csv(psu_div_forPlot, "psu_div_forPlot.csv")
# boxplots look better for diversity metrics than barplots (the opposite is true for biomass and abundance)...skip barplots
# by STRAT
#richness
#ggplot(psu_div_forPlot, aes(x=STRAT, y=richness_mean)) + geom_bar(position=position_dodge(), stat="identity") + geom_errorbar(aes(ymin=richness_mean-richness_se, ymax=richness_mean+richness_se), width=.2, position=position_dodge(.9)) + theme_classic() + theme(axis.text.x = element_text(angle=90)) + facet_wrap(~YEAR, scales='free_y')
#ggsave(file="fk_MeanRichness_ByStratumForEachYear.pdf", path="big_csv/plots")
# by YEAR
#ggplot(psu_div_forPlot, aes(x=YEAR, y=richness_mean, group=YEAR)) + geom_bar(position=position_dodge(), stat="identity") + geom_errorbar(aes(ymin=richness_mean-richness_se, ymax=richness_mean+richness_se), width=.2, position=position_dodge(.9)) + theme_classic() + theme(axis.text.x = element_text(angle=90)) + facet_wrap(~STRAT, scales='free_y')
#ggsave(file="fk_MeanRichness_ByYearForEachStratum.pdf", path="big_csv/plots")
# by STRAT and YEAR
# richness
ggplot(psu_div_forPlot, aes(x=YEAR, y=richness_mean, colour=STRAT, group=STRAT)) + geom_point() + geom_line() + geom_errorbar(aes(ymin=richness_mean-richness_se, ymax=richness_mean+richness_se), width=.2)
ggsave(file="fk_MeanRichness_ByYearANDStratum.pdf", path="big_csv/plots")
# simpson
ggplot(psu_div_forPlot, aes(x=YEAR, y=simpson_mean, colour=STRAT, group=STRAT)) + geom_point() + geom_line() + geom_errorbar(aes(ymin=simpson_mean-simpson_se, ymax=simpson_mean+simpson_se), width=.2)
ggsave(file="fk_MeanSimpson_ByYearANDStratum.pdf", path="big_csv/plots")
# shannon
ggplot(psu_div_forPlot, aes(x=YEAR, y=shannon_mean, colour=STRAT, group=STRAT)) + geom_point() + geom_line() + geom_errorbar(aes(ymin=shannon_mean-shannon_se, ymax=shannon_mean+shannon_se), width=.2)
ggsave(file="fk_MeanShannon_ByYearANDStratum.pdf", path="big_csv/plots")

# do this for dataframe with subset of years as well
# summarize data to get mean and standard error for each year/stratum combination  
psu_div_forPlot_YRsubset = psu_div_YRsubset %>% 
  group_by(YEAR, STRAT) %>%
  summarize(
    richness_mean = mean(richness),
    richness_n = length(richness),
    richness_sd = sd(richness),
    richness_se = richness_sd / sqrt(richness_n),
    richness_min = min(richness),
    richness_max = max(richness),
    simpson_mean = mean(simpson),
    simpson_n = length(simpson),
    simpson_sd = sd(simpson),
    simpson_se = simpson_sd / sqrt(simpson_n),
    simpson_min = min(simpson),
    simpson_max = max(simpson),
    shannon_mean = mean(shannon),
    shannon_n = length(shannon),
    shannon_sd = sd(shannon),
    shannon_se = shannon_sd / sqrt(shannon_n),
    shannon_min = min(shannon),
    shannon_max = max(shannon))
# boxplots look better for diversity metrics than barplots (the opposite is true for biomass and abundance)...skip barplots
# by STRAT
#ggplot(psu_div_forPlot_YRsubset, aes(x=STRAT, y=richness_mean)) + geom_bar(position=position_dodge(), stat="identity") + geom_errorbar(aes(ymin=richness_mean-richness_se, ymax=richness_mean+richness_se), width=.2, position=position_dodge(.9)) + theme_classic() + theme(axis.text.x = element_text(angle=90)) + facet_wrap(~YEAR, scales='free_y')
#ggsave(file="fk_MeanRichness_ByStratumForEachYear_YRsubset.pdf", path="big_csv/plots")
# by YEAR
#ggplot(psu_div_forPlot_YRsubset, aes(x=YEAR, y=richness_mean, group=YEAR)) + geom_bar(position=position_dodge(), stat="identity") + geom_errorbar(aes(ymin=richness_mean-richness_se, ymax=richness_mean+richness_se), width=.2, position=position_dodge(.9)) + theme_classic() + theme(axis.text.x = element_text(angle=90)) + facet_wrap(~STRAT, scales='free_y')
#ggsave(file="fk_MeanRichness_ByYearForEachStratum_YRsubset.pdf", path="big_csv/plots")
# by STRAT and YEAR
## richness
ggplot(psu_div_forPlot_YRsubset, aes(x=YEAR, y=richness_mean, colour=STRAT, group=STRAT)) + geom_point() + geom_line() + geom_errorbar(aes(ymin=richness_mean-richness_se, ymax=richness_mean+richness_se), width=.2)
ggsave(file="fk_MeanRichness_ByYearANDStratum_YRsubset.pdf", path="big_csv/plots")
## simpson
ggplot(psu_div_forPlot_YRsubset, aes(x=YEAR, y=richness_mean, colour=STRAT, group=STRAT)) + geom_point() + geom_line() + geom_errorbar(aes(ymin=richness_mean-richness_se, ymax=richness_mean+richness_se), width=.2)
ggsave(file="fk_MeanSimpson_ByYearANDStratum_YRsubset.pdf", path="big_csv/plots")
## shannon
ggplot(psu_div_forPlot_YRsubset, aes(x=YEAR, y=richness_mean, colour=STRAT, group=STRAT)) + geom_point() + geom_line() + geom_errorbar(aes(ymin=richness_mean-richness_se, ymax=richness_mean+richness_se), width=.2)
ggsave(file="fk_MeanShannon_ByYearANDStratum_YRsubset.pdf", path="big_csv/plots")

### although boxplots don't work well for biomass and abundance, they look great for diversity metrics
# richness
# by STRAT
ggplot(data = psu_div) + geom_boxplot(mapping = aes(x = STRAT, y = richness)) + facet_wrap(~YEAR, scales="free_y")
ggsave(file="fk_Richness_boxplot_ByStratumForEachYear.pdf", path="big_csv/plots")
# by YEAR
ggplot(data = psu_div) + geom_boxplot(mapping = aes(x = YEAR, y = richness, group = YEAR)) + facet_wrap(~STRAT, scales="free_y")
ggsave(file="fk_Richness_boxplot_ByYearForEachStratum.pdf", path="big_csv/plots")
# simpson
# by STRAT
ggplot(data = psu_div) + geom_boxplot(mapping = aes(x = STRAT, y = simpson)) + facet_wrap(~YEAR, scales="free_y")
ggsave(file="fk_Simpson_boxplot_ByStratumForEachYear.pdf", path="big_csv/plots")
# by YEAR
ggplot(data = psu_div) + geom_boxplot(mapping = aes(x = YEAR, y = simpson, group = YEAR)) + facet_wrap(~STRAT, scales="free_y")
ggsave(file="fk_Simpson_boxplot_ByYearForEachStratum.pdf", path="big_csv/plots")
# shannon
# by STRAT
ggplot(data = psu_div) + geom_boxplot(mapping = aes(x = STRAT, y = shannon)) + facet_wrap(~YEAR, scales="free_y")
ggsave(file="fk_Shannon_boxplot_ByStratumForEachYear.pdf", path="big_csv/plots")
# by YEAR
ggplot(data = psu_div) + geom_boxplot(mapping = aes(x = YEAR, y = shannon, group = YEAR)) + facet_wrap(~STRAT, scales="free_y")
ggsave(file="fk_Shannon_boxplot_ByYearForEachStratum.pdf", path="big_csv/plots")

# do this for dataframe with subset of years as well
# richness
# by STRAT
ggplot(data = psu_div_YRsubset) + geom_boxplot(mapping = aes(x = STRAT, y = richness)) + facet_wrap(~YEAR, scales="free_y")
ggsave(file="fk_Richness_boxplot_ByStratumForEachYear_YRsubset.pdf", path="big_csv/plots")
# by YEAR
ggplot(data = psu_div_YRsubset) + geom_boxplot(mapping = aes(x = YEAR, y = richness, group = YEAR)) + facet_wrap(~STRAT, scales="free_y")
ggsave(file="fk_Richness_boxplot_ByYearForEachStratum_YRsubset.pdf", path="big_csv/plots")
# simpson
# by STRAT
ggplot(data = psu_div_YRsubset) + geom_boxplot(mapping = aes(x = STRAT, y = simpson)) + facet_wrap(~YEAR, scales="free_y")
ggsave(file="fk_Simpson_boxplot_ByStratumForEachYear_YRsubset.pdf", path="big_csv/plots")
# by YEAR
ggplot(data = psu_div_YRsubset) + geom_boxplot(mapping = aes(x = YEAR, y = simpson, group = YEAR)) + facet_wrap(~STRAT, scales="free_y")
ggsave(file="fk_Simpson_boxplot_ByYearForEachStratum_YRsubset.pdf", path="big_csv/plots")
# shannon
# by STRAT
ggplot(data = psu_div_YRsubset) + geom_boxplot(mapping = aes(x = STRAT, y = shannon)) + facet_wrap(~YEAR, scales="free_y")
ggsave(file="fk_Shannon_boxplot_ByStratumForEachYear_YRsubset.pdf", path="big_csv/plots")
# by YEAR
ggplot(data = psu_div_YRsubset) + geom_boxplot(mapping = aes(x = YEAR, y = shannon, group = YEAR)) + facet_wrap(~STRAT, scales="free_y")
ggsave(file="fk_Shannon_boxplot_ByYearForEachStratum_YRsubset.pdf", path="big_csv/plots")

```

## Statistical analysis 

- Are diversity values and strata related? 
- Are doversoty values and subregions related? 
- Are diversity values and protection level related?

```{r stat libraries}

library(MASS) # contains boxcox function
library(moments) # contains skewness function
library(DescTools)
library(rpsychi) #statistical tests using summary data
library(AICcmodavg) #AIC
library(ade4) #mantel.rtest
library(ggplot2) #geom_boxplot and other plots
library(nortest) #checks for normality 
library(psych) #pairs.panels 
library(tidyverse)
library(magrittr) # needed to call this library separately from one in tidyverse in order to get functionality of github version not found in CRAN version
```

```{r bind psu metrics and coordinates}

if(!file.exists("psu_metrics_coord_rds")){
  psu_abun = read_rds("psu_abun_sum_rds") #5,241*4
  psu_bio = read_rds("psu_bio_sum_rds") #5,241*5
  psu_div = read_rds("psu_div_rds") # #5,241*7
  
  psu_abun_sum = as.data.frame(psu_abun$abundance_sum)
  psu_bio_sum = as.data.frame(psu_bio$biomass_sum)
  
  all_metrics = bind_cols(psu_div, psu_bio_sum, psu_abun_sum) 
  #dim(all_metrics)  5,241*9
  
  RVCdata_FK = read_rds("big_csv/RVCdata_FK.rds")
  FK_sample_data = RVCdata_FK$sample_data
  
  psu_coord = FK_sample_data %>% #4,420,893*22
    select(YEAR, PRIMARY_SAMPLE_UNIT, STRAT, PROT, LAT_DEGREES, LON_DEGREES) %>% #HABITAT_CD, ZONE_NR, MPA_NR 
    dplyr::group_by(YEAR, PRIMARY_SAMPLE_UNIT, STRAT, PROT) %>% #5,241 groups 
    summarize(
      LAT_DEGREES = first(LAT_DEGREES),
      LON_DEGREES = first(LON_DEGREES))
  
  psu_lat = as.data.frame(psu_coord$LAT_DEGREES)
  psu_long = as.data.frame(psu_coord$LON_DEGREES)
    
  psu_metric_coord = bind_cols(all_metrics, psu_lat, psu_long) %>%
    mutate(
       bio_sum = `psu_bio$biomass_sum`,
       abun_sum = `psu_abun$abundance_sum`,
       latitude = `psu_coord$LAT_DEGREES`,
       longitude = `psu_coord$LON_DEGREES`) %>%
    select(-`psu_bio$biomass_sum`,-`psu_coord$LAT_DEGREES`,-`psu_coord$LON_DEGREES`,-`psu_abun$abundance_sum`)

  saveRDS(psu_metric_coord, "psu_metrics_coord_rds")
}
else{
  psu_metric_coord = read_rds("psu_metrics_coord_rds")
}

```

Plot each metric by strata and subregion through time 

```{r Diversity by subregion and strata} 

#Generate figure legend based on FKNMS map 
#Read in shapefile and fortify
if (!file.exists('fk.shp.fortify_rds')){
fk.shp = readOGR( dsn = "FKNMS_sample_grid", layer = "FlaKeys_Grid") 
fk.shp.fortify = fortify(fk.shp) 
saveRDS(fk.shp.fortify, 'fk.shp.fortify_rds')
} 
else{ 
  fk.shp.fortify = readRDS('fk.shp.fortify_rds')
}

psu_metric_coord = read_rds("psu_metrics_coord_rds")

#Create data frame for legend points and titles
data=data.frame(labels=c("Lower","Middle","Upper"),
                shape=c(15:18,6),color=c("grey15","grey30","grey45"),
                long=c(-76.15,-76.135,-76.21,-76.41,-76.34),
                lat=c(37.2,37.7,38.15,38.67,39.16),
                lat2=c(37.08,37.54,38.03,38.51,39.04))
data$labels=factor(data$labels,levels=c("Lower","Middle","Upper"))

#Generate map legend for inset
inset.map=
  ggplot()+
  #Plot outline of Florida Keys 
  geom_polygon(data=subset(fk.shp.fortify,group==0.1),aes(x=long,y=lat,group=group),col="grey50",fill="white",lwd=0.4)+
  geom_segment(aes(x=-75.98281,y=37.39372,xend=-76.24769,yend=37.39372),col="grey50",lwd=1)+
  geom_segment(aes(x=-76.24572,y=37.87455,xend=-76.01332,yend=37.87455),col="grey50",lwd=1)+
  geom_segment(aes(x=-76.41753,y=38.35538,xend=-76.26395,yend=38.35538),col="grey50",lwd=1)+
  geom_segment(aes(x=-76.49554,y=38.83621,xend=-76.35090,yend=38.83621),col="grey50",lwd=1)+
  #Add legend points
  geom_point(data=data,aes(x=long,y=lat,shape=labels,color=labels),size=8)+
  scale_color_manual(values=c("grey45","grey30","grey15"))+
  scale_shape_manual(values=c(6,18:15))+
  #Add legend labels
  geom_text(data=data,aes(x=long,y=lat2,label=labels),size=4.6,fontface="bold")+
  theme(
    plot.margin=unit(c(0,0,0,0),"cm"),
    panel.background=element_blank(),
    legend.position="none",panel.border=element_blank(),
    panel.grid.major=element_blank(),panel.grid.minor=element_blank(),
    axis.title.x=element_blank(),axis.title.y=element_blank(),
    axis.text.x=element_blank(),axis.text.y=element_blank(),
    axis.line=element_blank(),axis.ticks=element_blank())  

#Plot diversity by region, strata and year for each diversity index
lapply(alphadiv.list,function(i) {
  #Melt dataframe so diversity indices are in a single column
  x=melt(i,id.vars=c("Month","Year","Reg"),measure.vars=c("abundance", "biomass", "richness","simpson","shannon", "raoQ"))
  #Summarize means and SEs by month, region, and variable
  x=ddply(x,c("Month","Reg","variable"),summarize,value.mean=mean(value),value.se=std.error(value))
  #Rename levels for figure plotting
  levels(x$variable)=c("A) Abundance","B) Biomass","C) Richness ","D) Shannon","E) Gini-Simpson","F) Functional")
  #Rename regions for figure plotting
  x$Reg=factor(x$Reg); levels(x$Reg)=c("Upper","Middle","Lower")
  #Plot line graph
  p=ggplot(x,aes(x=as.factor(Month),y=value.mean,col=as.factor(Reg),shape=as.factor(Reg)))+
    #annotation_custom(grob=inset.map,xmin=19,xmax=22.5,ymin=0,ymax=1.5)+
    geom_line(aes(group=as.factor(Reg)),lwd=1)+geom_point(size=5)+
    scale_color_manual(values=c("grey15","grey30","grey45","grey60","black"),name="Region")+
    scale_shape_manual(values=c(15:18,6),name="Region")+
    geom_errorbar(aes(ymax=value.mean+value.se,ymin=value.mean-value.se),width=0.1)+facet_wrap(~variable,scales="free")+
    labs(x="",y="Diversity")+theme_bw(base_size=16)+
    theme(
      plot.margin=unit(c(0,1,0,0),"cm"),legend.position="none",
      panel.grid.major=element_blank(),panel.grid.minor=element_blank(),panel.margin=unit(0.8,"lines"),
      strip.background=element_blank(),strip.text.x=element_text(size=18,hjust=0))
  #Add legend map
  grid.arrange(p,inset.map,nrow=1,widths=c(1,0.2))
} )

#Save PDF (13" x 8")
```

Examine spatial and temporal trends in the different dimensions of reef fish diversity. 

- Interpolate diversity for each strata/region/year groupings using kriging 
```{r Kriging}

library(parallel) #clusterExport 

#Create new list of data frames to convert to SpatialPointsDataFrame
#Convert class("dataframe") into class("SpatialPointsDataFrame")
#Project lat long coordinates onto an ellipse
#Transform the projection into cartesian coordinates

#Create a spatial grid onto which interpolated values can be plotted
#read in shapefile 
fk.shp = readOGR( dsn = "FKNMS_sample_grid", layer = "FlaKeys_Grid") 
#Generate interpolation grid
fknms.shp=spTransform(fk.shp,CRS("+proj=utm +zone=18 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0 ")) 
grd=spsample(fknms.shp,type="regular",n=2500,nsig=2) #n is sample size
par(mfrow=c(1,1))
plot(grd)

#Send interpolation grid to each core
parallel::clusterExport(cl=NULL, varlist=c("grd"))
#Error in defaultCluster(cl) : no cluster 'cl' supplied and none is registered

psu_indices= read_rds("psu_metrics_coord_rds")
psu_indices = as.matrix(psu_indices)

#Perform kriging interpolation for each matrix
#responses are log transformed to better meet assumptions of normality of errors and constant variance.
krigeall.list=parLapply(cl,psu_indices,function(i) { #cl=Null, X, function
  lapply(names(i@data)[66:71],function(j) { #X, function (i think giving names from data columns 66:71??)
    as.data.frame(autoKrige(formula(paste("log10(",j,")~1")),i,grd)$krige_output) 
    } ) 
  } ) 

#Apply names for titling during plotting
#names(krigeall.list)=c("abund","biomass","pres.abs")
for(i in seq_along(krigeall.list)) {
  names(krigeall.list[[i]])=c("mean.abundance", "mean.biomass" "richness", "shannon","simpspn","functional") 
  }  

#Plot all components across all years 
lapply(seq_along(krigeall.list),function(i) {
  do.call(grid.arrange,lapply(seq_along(krigeall.list[[i]]),function(j) {
    ggplot()+
      geom_raster(data=krigeall.list[[i]][[j]],aes(x=x1,y=x2,fill=var1.pred))+
      scale_fill_gradientn(colours=rev(rainbow(3)))+
      coord_equal()+labs(x="",y="",title=paste(names(krigeall.list[[i]])[j]))+ theme_bw()+theme(plot.margin=unit(c(0,0,0,0),"cm"),
                                                                                                axis.text.x=element_blank(),axis.text.y=element_blank(),
                                                                                                axis.ticks=element_blank(),legend.title=element_blank()) 
    } ) ) 
  } )

#Now repeat but instead interpolate by grouped years
krigemonth.list=parLapply(cl,alphasp.list,function(i) {
  lapply(names(i@data)[66:71],function(j) {
    lapply(unique((i@data)$STRAT),function(k) {
      as.data.frame(autoKrige(formula(paste("log(",j,")~1")),subset(i,i@data$STRAT==k),grd)$krige_output)  
      } ) 
    } ) 
  } )

#Apply names for titling during plotting
for(i in seq_along(krigemonth.list)) {
  names(krigemonth.list[[i]])=c("A) Mean Abundance","B) Mean Biomass","C) Richness","D) Shannon","E) Gini-Simpson","F) Functional") 
  }
for(i in seq_along(krigemonth.list)) {
  for(j in seq_along(krigemonth.list[[i]])) {
    names(krigemonth.list[[i]][[j]])=c("1999-2004","2005-2010","2011-2016") #change to year groupings 
  } 
}

#Collapse lists into data frame
krigemonth.list=lapply(krigemonth.list,function(i) lapply(i,function(j) do.call(rbind,j)) )

#Split rownames into month value and bind to dataframe
krigemonth.list=lapply(krigemonth.list,function(i) {
  lapply(i,function(j) {
    cbind(data.frame(str_split_fixed(rownames(j),"\\.",2)[,1]),j) } ) } )
for(i in seq_along(krigemonth.list)) {
  for(j in seq_along(krigemonth.list[[i]])) { names(krigemonth.list[[i]][[j]])[1]=c("month") } }
#Order levels of month for facetting
for(i in seq_along(krigemonth.list)) {
  for(j in seq_along(krigemonth.list[[i]])) { 
    krigemonth.list[[i]][[j]]$month=factor(krigemonth.list[[i]][[j]]$month,levels=c("Mar","May","July","Sept","Nov")) } }

#Plot and facet by month (12" x 18")
lapply(seq_along(krigemonth.list),function(i) {
  do.call(grid.arrange,c(lapply(seq_along(krigemonth.list[[i]]),function(j) {
    if(j!=2) {
      ggplot()+
        geom_raster(data=krigemonth.list[[i]][[j]],aes(x=x1,y=x2,fill=exp(var1.pred)))+
        scale_fill_gradientn(colours=rev(rainbow(3)))+
        facet_wrap(~month,nrow=1)+
        coord_equal()+labs(x="",y="",title=names(krigemonth.list[[i]])[j])+
        theme_bw()+theme(plot.margin=unit(c(0,0,0,0),"cm"),
                   plot.title=element_text(size=18,hjust=0,vjust=1),
                   strip.text=element_text(size=12),
                   panel.grid.major=element_blank(),panel.grid.minor=element_blank(),
                   axis.text.x=element_blank(),axis.text.y=element_blank(),
                   axis.ticks=element_blank(),legend.title=element_blank()) 
    } else {
      ggplot()+
        geom_raster(data=krigemonth.list[[i]][[j]],aes(x=x1,y=x2,fill=sin(var1.pred)^2))+
        scale_fill_gradientn(colours=rev(rainbow(3)))+
        facet_wrap(~month,nrow=1)+
        coord_equal()+labs(x="",y="",title=names(krigemonth.list[[i]])[j])+
        theme_bw()+theme(plot.margin=unit(c(0,0,0,0),"cm"),
                     plot.title=element_text(size=18,hjust=0,vjust=1),
                     strip.text=element_text(size=12),
                     panel.grid.major=element_blank(),panel.grid.minor=element_blank(),
                     axis.text.x=element_blank(),axis.text.y=element_blank(),
                     axis.ticks=element_blank(),legend.title=element_blank()) }
  } ), list(ncol=2)) )
} )

```

Build GAM model with temporal, spatial, environmental and management variables 

$y_i = \alpha + f_1(X_1i) + f_2(X_2i) + g_1(X_3i) + g_2(X_4i) + \epsilon_i$
$y_i = \alpha + f_1 (YEAR)+ f_2(PROT) + g_1(LONG, LAT)* YEAR + g_2(STRAT) + \epsilon_i$

$y_i$ is the calculated diversity for station i (response variable)
$\alpha$ is the intercept that scales the model prediction to the appropriate level of the response
$f_1 (X_1i)$ is a smoother
$g_1 (X_1i)$ is nonparametric smoother for covariates strata
$\epsilon_i$ the residual error

Temporal variables:
- YEAR: Year (categorical factor)

Spatial variables:
- latitude and longitude (continuous covariate)

Environmental variables: 
- strata (categorical factor)
- Rugosity (categorical factor)
- bottom depth (continuous covariate)
- vertical relief (continuous covariate)
- % coral cover (continuous covariate)
- % hard bottom (continuous covariate)

Management variable:
- PROT: No take marine reserve  (categorical factor)

Conitinuous covariates: modeled using non-parametric smoothing 
- thin plate regression splines with shrinkage terms (Ciannelli et al., 2008)

Categorical factors: models parametrically to determine their mean effect sizes (Zurr et al., 2009)

```{r strata environmental characteristics}

# read in strata data for each year and psu 
#year, psu, latitude, longitude, rugosity, bottom_depth, vertical_relief, percent_coral_cover, percent_hardbottom

# bind to psu diversity dataframe 
```

- Extract and interpolate the residuals for each index and fit against each other 
```{r GAM}
library(rgdal) #readOGR, spTransform 

#Extract residuals for each metric vs richness, evenness, and species diversity after fitting GAM
resids.list=lapply(alphadiv.list,function(i) { cbind(i,do.call(cbind,
  lapply(list(
    "log(species.div)~log(richness)","log(taxo.div)~log(richness)","log(func.div)~log(richness)","log(phylo.div)~log(richness)",
    "log(species.div)~log(taxo.div)","log(species.div)~log(func.div)","log(species.div)~log(phylo.div)",
    "log(func.div)~log(species.div)","log(func.div)~log(taxo.div)","log(func.div)~log(phylo.div)",
    "log(phylo.div)~log(species.div)","log(phylo.div)~log(taxo.div)","log(phylo.div)~log(func.div)",
    "log(taxo.div)~log(species.div)","log(taxo.div)~log(func.div)","log(taxo.div)~log(phylo.div)"),
    function(j) { res=as.data.frame(residuals(gam(formula(paste(j)),data=i))); return(res) } ) ) ) } )

#Replace column names for residuals
for(i in seq_along(resids.list)) { colnames(resids.list[[i]])[74:89]=c(
  "species.div.richness","taxo.div.richness","func.div.richness","phylo.div.richness",
  "species.div.taxo.div","species.div.func.div","species.div.phylo.div",
  "func.div.species.div","func.div.taxo.div","func.div.phylo.div",
  "phylo.div.species.div","phylo.div.taxo.div","phylo.div.func.div",
  "taxo.div.species.div","taxo.div.func.div","taxo.div.phylo.div") }
                   
#Save residuals as another object for mapping
residssp.list=resids.list
#Convert class("dataframe") into class("SpatialPointsDataFrame")
for(i in seq_along(residssp.list)) { coordinates(residssp.list[[i]])=~Long+Lat } 
lapply(residssp.list,class)
#Project latlong coordinates onto an ellipse
for(i in seq_along(residssp.list)) { 
  proj4string(residssp.list[[i]])="+proj=longlat +ellps=GRS80 +datum=NAD83 +no_defs" }
#Transform the projection into cartesian coordinates
residssp.list=lapply(seq_along(residssp.list),function(i) {
  spTransform(residssp.list[[i]],CRS("+proj=utm +zone=18 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0")) } )
# 
# #Interpolate across all months and years by passing each matrix to a different core 
# #Perform kriging interpolation for each matrix in the list (abundance- and biomass-weighted))
# krigeall.resids.list=parLapply(cl,resids.list,function(i) {
#   lapply(names(i@data)[16:19],function(j) {
#     as.data.frame(autoKrige(formula(paste(j,"~1")),i,grd)$krige_output) } ) } )
# 
# #Apply names for titling during plotting
# names(krigeall.resids.list)=c("abund","biomass")
# for(i in seq_along(krigeall.resids.list)) {
#   names(krigeall.resids.list[[i]])=c("species.div.resids","taxo.div.resids","func.div.resids","phylo.div.resids") }  
# 
# #Plot all components across all years and months
# lapply(seq_along(krigeall.resids.list),function(i) {
#   do.call(grid.arrange,lapply(seq_along(krigeall.resids.list[[i]]),function(j) {
#     ggplot()+
#       geom_raster(data=krigeall.resids.list[[i]][[j]],aes(x=x1,y=x2,fill=var1.pred))+
#       scale_fill_gradientn(colours=rev(rainbow(3)))+
#       coord_equal()+labs(x="",y="",title=paste(names(krigeall.resids.list[[i]])[j]))+
#       theme_bw()+theme(plot.margin=unit(c(0,0,0,0),"cm"),
#                        axis.text.x=element_blank(),axis.text.y=element_blank(),
#                        axis.ticks=element_blank(),legend.title=element_blank()) } ) ) } )

#Interpolate across by month across all years by passing each matrix to a different core 
#Perform kriging interpolation for each matrix in the list (abundance- and biomass-weighted))
krigemonth.resids.list=parLapply(cl,residssp.list,function(i) {
  lapply(names(i@data)[72:87],function(j) {
    lapply(unique((i@data)$Month),function(k) {
      as.data.frame(autoKrige(formula(paste(j,"~1")),subset(i,i@data$Month==k),grd)$krige_output) 
} ) } ) } )

#Apply names for titling during plotting
names(krigemonth.resids.list)=c("abund","biomass","pres.abs")

for(i in seq_along(krigemonth.resids.list)) {
  names(krigemonth.resids.list[[i]])=c(
    "Gini-Simpson~Richness","Taxonomic~Richness","Functional~Richness","Phylogenetic~Richness",
    "Gini-Simpson~Taxonomic","Gini-Simpson~Functional","Gini-Simpson~Phylogenetic",
    "Functional~Gini-Simpson","Functional~Taxonomic","Functional~Phylogenetic",
    "Phylogenetic~Gini-Simpson","Phylogenetic~Taxonomic","Phylogenetic~Functional",
    "Taxonomic~Gini-Simpson","Taxonomic~Functional","Taxonomic~Phylogenetic") }

for(i in seq_along(krigemonth.resids.list)) {
  for(j in seq_along(krigemonth.resids.list[[i]])) {
    names(krigemonth.resids.list[[i]][[j]])=c("Mar","May","July","Sept","Nov") } }
#Collapse lists into data frame
krigemonth.resids.list=lapply(krigemonth.resids.list,function(i) lapply(i,function(j) do.call(rbind,j)) ) 
#Split rownames into month value and bind to dataframe
krigemonth.resids.list=lapply(krigemonth.resids.list,function(i) {
  lapply(i,function(j) {
    cbind(data.frame(str_split_fixed(rownames(j),"\\.",2)[,1]),j) } ) } )
for(i in seq_along(krigemonth.resids.list)) {
  for(j in seq_along(krigemonth.resids.list[[i]])) { names(krigemonth.resids.list[[i]][[j]])[1]=c("month") } }
#Order levels of month for facetting
for(i in seq_along(krigemonth.resids.list)) {
  for(j in seq_along(krigemonth.resids.list[[i]])) { 
    krigemonth.resids.list[[i]][[j]]$month=factor(krigemonth.resids.list[[i]][[j]]$month,
                                                  levels=c("Mar","May","July","Sept","Nov")) } }

#Plot and facet by month (12.5" x 20")
lapply(seq_along(krigemonth.resids.list),function(i) {
  do.call(grid.arrange,c(lapply(seq_along(krigemonth.resids.list[[i]]),function(j) {
    ggplot()+
      geom_raster(data=krigemonth.resids.list[[i]][[j]],aes(x=x1,y=x2,fill=exp(var1.pred)))+
      scale_fill_gradientn(colours=rev(rainbow(3)))+
      facet_wrap(~month,nrow=1)+
      coord_equal()+labs(x="",y="",title=paste(names(krigemonth.resids.list[[i]])[j]))+
      theme_bw()+theme(plot.margin=unit(c(0,0,0,0),"cm"),
                       panel.grid.major=element_blank(),panel.grid.minor=element_blank(),
                       axis.text.x=element_blank(),axis.text.y=element_blank(),
                       axis.ticks=element_blank(),legend.title=element_blank()) } ), list(ncol=3)) ) } )

#Repeat but only for subset for figure in main text
figure3.subset=krigemonth.resids.list[[2]][c("Gini-Simpson~Richness","Functional~Phylogenetic","Functional~Taxonomic","Phylogenetic~Taxonomic")]
names(figure3.subset)=c("A) Gini-Simpson~Richness","B) Functional~Phylogenetic","C) Functional~Taxonomic","D) Phylogenetic~Taxonomic")
#Generate figure ("12 x 12")
do.call(grid.arrange,c(lapply(seq_along(figure3.subset),function(j) {
  ggplot()+
    geom_raster(data=figure3.subset[[j]],aes(x=x1,y=x2,fill=exp(var1.pred)))+
    scale_fill_gradientn(colours=rev(rainbow(3)))+
    facet_wrap(~month,nrow=1)+
    coord_equal()+labs(x="",y="",title=paste(names(figure3.subset)[j]))+
    theme_bw()+theme(plot.margin=unit(c(0.2,0,0,0),"cm"),
                     plot.title=element_text(size=18,hjust=0,vjust=1),
                     strip.text=element_text(size=12),
                     panel.grid.major=element_blank(),panel.grid.minor=element_blank(),
                     axis.text.x=element_blank(),axis.text.y=element_blank(),
                     axis.ticks=element_blank(),legend.title=element_blank()) } ), list(ncol=2))) 
```

- fit GAM to each index separately. 
```{r DRIVERS OF PATTERNS IN DIVERSITY}

#Run generalized additive models to identify the contribution of each predictor to % deviance explained
#Code modified from: https://stat.ethz.ch/pipermail/r-help/2011-November/295324.html

#Send alphadiv.list to each core
clusterExport(cl,varlist=c("alphadiv.list"))

#Compute models and store summary table of partial deviances in a list
partial.deviance.list=parLapply(cl,seq_along(alphadiv.list),function(i) {
  do.call(rbind,lapply(list("richness","evenness","species.div","func.div","phylo.div","taxo.div"),function(j) {

  #Subset only columns that are necessary to run the models and omit any NAs so all models are run on the same data
  data=na.omit(alphadiv.list[[i]][,c("Month","Year","Long","Lat","SA","WT","DO","TDEPTH","richness","evenness","species.div","func.div","phylo.div","taxo.div")])
  
  #Build full model and null model based on (transformed) response variable
  if(j=="richness") {
    fullmod=gam(formula(paste(j,"~Month+Year+s(Long,Lat,by=Month,bs='ts')+s(SA,bs='ts')+s(WT,bs='ts')+
                  s(DO,bs='ts')+s(TDEPTH,bs='ts')")),family=poisson,data=data)
    nullmod=gam(formula(paste(j,"~1")),family=poisson,data=data)
  } else if(j=="evenness") {
    fullmod=gam(formula(paste("asin(sqrt(",j,"))~Month+Year+s(Long,Lat,by=Month,bs='ts')+s(SA,bs='ts')+s(WT,bs='ts')+
                  s(DO,bs='ts')+s(TDEPTH,bs='ts')")),data=data)
    nullmod=gam(formula(paste("asin(sqrt(",j,"))~1")),data=data)
  } else {
    fullmod=gam(formula(paste("log(",j,")~Month+Year+s(Long,Lat,by=Month,bs='ts')+s(SA,bs='ts')+s(WT,bs='ts')+
                  s(DO,bs='ts')+s(TDEPTH,bs='ts')")),data=data)
    nullmod=gam(formula(paste("log(",j,")~1")),data=data) }

```

- To assess the explanatory power of each variable calculate the partial deviances by sequentially removing suites of predictors from the full gam model corresponding to indicators of space, time, environment, or management. 

- Repeat this for all possible permutations 

- Average the deviances for all models in which the predictor appeared and calculate the standard error 

- the partial deviances are the proportion of total explained deviance of the model uniquely explained by space, time, environment or management, accounting for the other predictors in the model. 

```{r partial deviances}
  #Set up empty vectors for deviances
  time.dev=c(); space.dev=c(); env.dev=c()
  
  #Sequence 1: time->space->environment
  time.dev[1]=
    (deviance(update(fullmod,.~.-Month-Year,sp=fullmod$sp))-
       deviance(fullmod))/deviance(nullmod)
  space.dev[1]=
    (deviance(update(fullmod,.~.-Month-Year-s(Long,Lat,by=Month,bs='ts'),sp=fullmod$sp[6:9]))-
      deviance(update(fullmod,.~.-Month-Year,sp=fullmod$sp)))/deviance(nullmod)
  env.dev[1]=
    (deviance(nullmod)-
       deviance(update(fullmod,.~.-Month-Year-s(Long,Lat,by=Month,bs='ts'),sp=fullmod$sp[6:9])))/deviance(nullmod)

  #Sequence 2: time->environment->space
  time.dev[2]=
    (deviance(update(fullmod,.~.-Month-Year,sp=fullmod$sp))-
       deviance(fullmod))/deviance(nullmod)
  env.dev[2]=
    (deviance(update(fullmod,.~.-Month-Year-s(SA,bs='ts')-s(WT,bs='ts')-s(DO,bs='ts')-s(TDEPTH,bs='ts'),sp=fullmod$sp[1:5]))-
       deviance(update(fullmod,.~.-Month-Year,sp=fullmod$sp)))/deviance(nullmod)
  space.dev[2]=
    (deviance(nullmod)-
       deviance(update(fullmod,.~.-Month-Year-s(SA,bs='ts')-s(WT,bs='ts')-s(DO,bs='ts')-s(TDEPTH,bs='ts'),sp=fullmod$sp[1:5])))/deviance(nullmod)
       
  #Sequence 3: space->time->environment
  space.dev[3]=
    (deviance(update(fullmod,.~.-s(Long,Lat,by=Month,bs='ts'),sp=fullmod$sp[6:9]))-
       deviance(fullmod))/deviance(nullmod)
  time.dev[3]=
    (deviance(update(fullmod,.~.-s(Long,Lat,by=Month,bs='ts')-Month-Year,sp=fullmod$sp[6:9]))-
       deviance(update(fullmod,.~.-s(Long,Lat,by=Month,bs='ts'),sp=fullmod$sp[6:9])))/deviance(nullmod)
  env.dev[3]=
    (deviance(nullmod)-
       deviance(update(fullmod,.~.-s(Long,Lat,by=Month,bs='ts')-Month-Year,sp=fullmod$sp[6:9])))/deviance(nullmod)
  
  #Sequence 4: space->environment->time
  space.dev[4]=
    (deviance(update(fullmod,.~.-s(Long,Lat,by=Month,bs='ts'),sp=fullmod$sp[6:9]))-
       deviance(fullmod))/deviance(nullmod)
  env.dev[4]=
    (deviance(update(fullmod,.~.-s(Long,Lat,by=Month,bs='ts')-s(SA,bs='ts')-s(WT,bs='ts')-s(DO,bs='ts')-s(TDEPTH,bs='ts')))-
       deviance(update(fullmod,.~.-s(Long,Lat,by=Month,bs='ts'),sp=fullmod$sp[6:9])))/deviance(nullmod)
  time.dev[4]=
    (deviance(nullmod)-
       deviance(update(fullmod,.~.-s(Long,Lat,by=Month,bs='ts')-s(SA,bs='ts')-s(WT,bs='ts')-s(DO,bs='ts')-s(TDEPTH,bs='ts'))))/deviance(nullmod)
  
  #Sequence 5: environment->time->space
  env.dev[5]=
    (deviance(update(fullmod,.~.-s(SA,bs='ts')-s(WT,bs='ts')-s(DO,bs='ts')-s(TDEPTH,bs='ts'),sp=fullmod$sp[1:5]))-
       deviance(fullmod))/deviance(nullmod)
  time.dev[5]=
    (deviance(update(fullmod,.~.-s(SA,bs='ts')-s(WT,bs='ts')-s(DO,bs='ts')-s(TDEPTH,bs='ts')-Month-Year,sp=fullmod$sp[1:5]))-
       deviance(update(fullmod,.~.-s(SA,bs='ts')-s(WT,bs='ts')-s(DO,bs='ts')-s(TDEPTH,bs='ts'),sp=fullmod$sp[1:5])))/deviance(nullmod)
  space.dev[5]=
    (deviance(nullmod)-
       deviance(update(fullmod,.~.-s(SA,bs='ts')-s(WT,bs='ts')-s(DO,bs='ts')-s(TDEPTH,bs='ts')-Month-Year,sp=fullmod$sp[1:5])))/deviance(nullmod)
       
  #Sequence 6: environment->space->time
  env.dev[6]=
    (deviance(update(fullmod,.~.-s(SA,bs='ts')-s(WT,bs='ts')-s(DO,bs='ts')-s(TDEPTH,bs='ts'),sp=fullmod$sp[1:5]))-
       deviance(fullmod))/deviance(nullmod)
  space.dev[6]=
    (deviance(update(fullmod,.~.-s(SA,bs='ts')-s(WT,bs='ts')-s(DO,bs='ts')-s(TDEPTH,bs='ts')-s(Long,Lat,by=Month,bs='ts')))-
       deviance(update(fullmod,.~.-s(SA,bs='ts')-s(WT,bs='ts')-s(DO,bs='ts')-s(TDEPTH,bs='ts'),sp=fullmod$sp[1:5])))/deviance(nullmod)
  time.dev[6]=
    (deviance(nullmod)-
       deviance(update(fullmod,.~.-s(SA,bs='ts')-s(WT,bs='ts')-s(DO,bs='ts')-s(TDEPTH,bs='ts')-s(Long,Lat,by=Month,bs='ts'))))/deviance(nullmod)
  
  #Bind results into data.frame
  prop.deviance=data.frame(
    Diversity=paste(j),
    Predictor=c("All","Space","Time","Environment"),
    Partial.Deviance=c(summary(fullmod)$dev.expl,mean(space.dev),mean(time.dev),mean(env.dev)),
    Partial.Deviance.SE=c(0,std.error(space.dev),std.error(time.dev),std.error(env.dev)) )
} ) ) } )
#Close the cluster
stopCluster(cl)

#Name objects in list
names(partial.deviance.list)=c("abund","biomass","pres.abs")
#Rename diversity levels for better plotting
for(i in seq_along(partial.deviance.list)) {
  partial.deviance.list[[i]]=transform(partial.deviance.list[[i]],Diversity=recode(Diversity,
    "'richness'='Richness';'evenness'='Evenness';'species.div'='Gini-Simpson';
     'func.div'='Functional';'phylo.div'='Phylogenetic';'taxo.div'='Taxonomic'")) }
#Reorder diversity levels for better plotting
for(i in seq_along(partial.deviance.list)) {
  partial.deviance.list[[i]][,"Diversity"]=factor(partial.deviance.list[[i]][,"Diversity"],
    levels=c("Richness","Evenness","Gini-Simpson","Functional","Phylogenetic","Taxonomic")) }

#Create barplot (8" x 6")
lapply(seq_along(partial.deviance.list),function(i) {
  data=subset(partial.deviance.list[[i]],Predictor!="All")
  ggplot(data,aes(x=Diversity,y=Partial.Deviance*100,fill=Predictor))+
    geom_bar(position="dodge",color="black")+
    geom_errorbar(aes(max=(Partial.Deviance+Partial.Deviance.SE)*100,
                      min=(Partial.Deviance-Partial.Deviance.SE)*100),
                  width=0.3,size=0.6,position=position_dodge(width=0.9))+
    labs(x="",y="Total % Deviance Explained\n")+
    scale_fill_manual(values=c("grey50","grey80","white"))+
    theme_bw(base_size=18)+theme(
      panel.grid.major=element_blank(),panel.grid.minor=element_blank(),
      legend.position="bottom",legend.title=element_blank()) } )

```

```{r practice GAM}
#Build full model and null model based on (transformed) response variable
  #s() smooth term in gam model 
  #bs() a two letter character string indicating the smoothing basis to use (ts plate regression splines)

# 1. Read in data and remove years (optional)
psu_metric_coord = read_rds("psu_metrics_coord_rds")

#Build full model and null model based on (transformed) response variable

  richness_fullmodel = gam(formula(richness~YEAR+STRAT+PROTs(latitude, longitude, by=YEAR, bs="ts")), family=poisson, data=psu_metric_coord)
  
  richness_nullmodel = gam(formula(richness~1), family=poisson, data=psu_metric_coord) 
  
  shannon_fullmodel = gam(formula(shannon~YEAR+STRAT+PROT+s(longitude,latitude, by=YEAR, bs="ts")), data=psu_metric_coord)
  shannon_nullmodel=gam(formula(shannon~1), data=psu_metric_coord)
  
  summary(shannon_fullmodel) #explained deviance (R2), variance of residuals, etc
  anova(shannon_fullmodel)#p-value and F-test
  gam.check(shannon_fullmodel) #produces graphical diagnosis for the model. Use to 1) assess normality with QQ plot and the histogram 2) homogeneity (residuals vs fitted values) 3) model fit (fitted values versus observed values). Ideally the response against fitted values should show a straight line. 

    rich_model = gam(formula(richness~YEAR+rugosity+s(longitude,latitude,by=YEAR,bs='ts')+s(depth,bs='ts')+s(vert_relief,bs='ts')+s(per_coral_cover,bs='ts')+s(per_hardbottom,bs='ts')), family=poisson),
    rich_nullmodel = gam(formula(richness,"~1"), family=poisson) 
    
    shan_model = gam(formula(shannon~YEAR+rugosity+s(longitude,latitude,by=YEAR,bs='ts')+s(depth,bs='ts')+s(vert_relief,bs='ts')+s(per_coral_cover,bs='ts')+s(per_hardbottom,bs='ts'))) 
    shannon_nullmodel = gam(formula("log(",shannon,")~1"))) 

mgcv::summary(gam$rich_model) #explained deviance (R2), variance of residuals, etc. 
mgcv::anova(gam$rich_model)#p-value and F-test
mgcv::plot(gam$rich_model) #produces fitted smoother
gam.check(gam$rich_model) #produces graphical diagnosis for the model. Use to 1) assess normality with QQ plot and the histogram 2) homogeneity (residuals vs fitted values) 3) model fit (fitted values versus observed values). Ideally the response against fitted values should show a straight line. 

#Check how well the model fits using graphical evidence or if it isn't clear with AIC, which measures goodness of fit and model complexity. The lower the AIC, the better the model fits the data. Can also look at GCV value in summary output. 
AIC(gam$rich_model)

```
